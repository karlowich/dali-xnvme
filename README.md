# Dali / xNVMe
This directory includes two almost identical examples of changing the data reader in a DALI pipeline to use an external source.
Specifically we are interested in replacing the `fn.readers.file()` data reader with `fn.external_source()`.
The goal is a comparison of 4 different DALI pipelines:

- [x] Using `fn.readers.file()`
- [x] Using an external source with Python file API
- [x] Using an external source with xNVMe file API
- [x] Using an external source with xNVMe block API (GDS-like or BaM-like)

## More about the DALI file reader
`fn.readers.file()` expects a directory with the following structure:
```
  <file_root>/a/img0.jpg
  <file_root>/a/img1.jpg
  <file_root>/b/img2.jpg
  <file_root>/b/img3.jpg
  <file_root>/c/img4.jpg
  <file_root>/c/img5.jpg
```
From this the output will be `(file, label)`:
```
  (<file_root>/a/img0.jpg, 0) 
  (<file_root>/a/img1.jpg, 0)
  (<file_root>/b/img2.jpg, 1)
  (<file_root>/b/img3.jpg, 1)
  (<file_root>/c/img4.jpg, 2)
  (<file_root>/c/img5.jpg, 2)
```
As such, the labels are derived from the sorting of subdirectories.
This is behaviour we need to mimic with the external source for the implementation to work in a plug-n-play manner.

## Setup
- Install dependency
  * https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html
- Build docker container
  * `docker build . -t dali-xnvme`
- Run docker container
  * The dataset (fx. imagenet) should reside in a directory, `{datadir}`, with a `train` subdirectory.
  * The `-p` allows the jupyter instance to be opened in a browser on a host machine
  * `docker run --rm --privileged --runtime nvidia -it -p 8888:8888 -v .:/workspace/dali-xnvme -v {datadir}:/data --ipc=host dali-xnvme`

### Block map
To use the xNVMe block API, a bmap.json file is required.
This can be generated by placing the dataset on a drive an running `python extentinfo/get_extent_info.py --mountpoint {datadir}/train/`.
Unfortunately, one has to manually remove prefixes from the keys in the output so the path looks like: `train/n01491361/n01491361_8657.JPEG`

## Benchmarks
To get the performance of different dataloading methods run `benchmark.py`.
Below are example configurations:
- `python benchmark.py --batchsize 64 --batches 1000 xnvme-gds --uri /dev/libnvm0 --mem cpu --bmap imagenet_train_bmap.json`
- `python benchmark.py --batchsize 64 --batches 1000 xnvme-gds --uri /dev/libnvm0 --mem gpu --bmap imagenet_train_bmap.json`
- `python benchmark.py --batchsize 64 --batches 1000 xnvme-bam --uri /dev/libnvm0 --bmap imagenet_train_bmap.json`
- `python benchmark.py --batchsize 64 --batches 1000 dali --datadir /data`
- `python benchmark.py --batchsize 64 --batches 1000 python-file --datadir /data`
- `python benchmark.py --batchsize 64 --batches 1000 xnvme-file --datadir /data`

## Jupyter
To get an idea of how the correctness of the code, images can be printed in the a jupyter notebook.
Note, the jupyter notebook can be unstable due to memory not being properly garbage collected / freed. 
- run `jupyter lab`
- open `dali-example.ipynb`
