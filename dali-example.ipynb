{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1245e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import cupy as cp\n",
    "from nvidia.dali.backend import TensorListGPU\n",
    "from nvidia.dali import pipeline_def, Pipeline\n",
    "import nvidia.dali.types as types\n",
    "import nvidia.dali.fn as fn\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ctypes import CFUNCTYPE, POINTER, c_uint8, c_void_p, byref, pointer, cast, c_uint32\n",
    "import xnvme.ctypes_bindings as xnvme\n",
    "from xnvme.ctypes_bindings.api import char_pointer_cast\n",
    "\n",
    "\n",
    "DATADIR = \"/data\"\n",
    "BMAP = \"imagenet_train_bmap.json\"\n",
    "BATCH_SIZE = 4 # batch size per GPU\n",
    "IMAGENET_MAX_SIZE = 15737107 # max size of images in the imagenet dataset\n",
    "GDS_QD = 63\n",
    "NLB = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79211688",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.nvidia.com/deeplearning/dali/user-guide/docs/examples/image_processing/decoder_examples.html\n",
    "def show_images(image_batch):\n",
    "    columns = 4\n",
    "    rows = BATCH_SIZE // (columns)\n",
    "    fig = plt.figure(figsize=(32, (32 // columns) * rows))\n",
    "    gs = gridspec.GridSpec(rows, columns)\n",
    "    for j in range(rows * columns):\n",
    "        plt.subplot(gs[j])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(image_batch.at(j))\n",
    "\n",
    "def show_pipeline_output(pipe):\n",
    "    pipe.build()\n",
    "    images, _ = pipe.run()\n",
    "    if isinstance(images, TensorListGPU):\n",
    "        images = images.as_cpu()\n",
    "    show_images(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4449215",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline_def\n",
    "def dali_pipe(datadir):\n",
    "    traindir = os.path.join(datadir, \"train\")\n",
    "    jpegs, labels = fn.readers.file(file_root=traindir,\n",
    "            shard_id=0,\n",
    "            num_shards=1,\n",
    "            random_shuffle=True,\n",
    "            pad_last_batch=True,\n",
    "            name=\"FILE\"\n",
    "        )\n",
    "    \n",
    "    images = fn.decoders.image(jpegs,\n",
    "        device=\"mixed\",\n",
    "        output_type=types.RGB,\n",
    "        device_memory_padding=211025920,\n",
    "        host_memory_padding=140544512,\n",
    "    )\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a936a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://github.com/NVIDIA/DALI/blob/main/docs/examples/frameworks/pytorch/pytorch-external_input.ipynb\n",
    "class FileInputIterator(object):\n",
    "    # This is essentially fn.readers.file reimplemented as an external source\n",
    "    def __init__(self, datadir, batch_size):\n",
    "        self.images_dir = os.path.join(datadir, \"train\")\n",
    "        self.batch_size = batch_size\n",
    "        dirs = {os.path.join(self.images_dir, d): i for i, d in enumerate(sorted(os.listdir(self.images_dir)))}\n",
    "        self.files = [(os.path.join(root, file), dirs[root]) for root, _, files in os.walk(self.images_dir) for file in files]\n",
    "        self.n = len(self.files)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        shuffle(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = []\n",
    "        labels = []\n",
    "\n",
    "        if self.i >= self.n:\n",
    "            self.__iter__()\n",
    "            raise StopIteration\n",
    "\n",
    "        for _ in range(self.batch_size):\n",
    "            jpeg, label = self.files[self.i % self.n]\n",
    "\n",
    "            batch.append(\n",
    "                np.fromfile(jpeg, dtype=np.uint8)\n",
    "            )\n",
    "\n",
    "            labels.append(\n",
    "                torch.tensor([label], dtype=torch.int32)\n",
    "            )\n",
    "\n",
    "            self.i += 1\n",
    "        return (batch, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    next = __next__\n",
    "    \n",
    "@pipeline_def\n",
    "def file_pipe(datadir):\n",
    "    pipe = Pipeline.current()\n",
    "    batch_size = pipe.max_batch_size\n",
    "\n",
    "    jpegs, labels = fn.external_source(\n",
    "        source=FileInputIterator(datadir=datadir, batch_size=batch_size), num_outputs=2, dtype=[types.UINT8, types.INT32]\n",
    "    )\n",
    "    images = fn.decoders.image(jpegs,\n",
    "        device=\"mixed\",\n",
    "        output_type=types.RGB,\n",
    "        device_memory_padding=211025920,\n",
    "        host_memory_padding=140544512,\n",
    "    )\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4bedd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNVMEFileInputIterator(object):\n",
    "    # This is implemented using the xNVMe file API\n",
    "    def __init__(self, datadir, batch_size):\n",
    "        self.images_dir = os.path.join(datadir, \"train\")\n",
    "        self.batch_size = batch_size\n",
    "        dirs = {os.path.join(self.images_dir, d): i for i, d in enumerate(sorted(os.listdir(self.images_dir)))}\n",
    "        self.files = [(os.path.join(root, file), dirs[root]) for root, _, files in os.walk(self.images_dir) for file in files]\n",
    "        opts = xnvme.xnvme_opts()\n",
    "        xnvme.xnvme_opts_set_defaults(byref(opts))\n",
    "        # Make tmp file to link buffers to\n",
    "        file_path = \"/tmp/xnvme_file\"\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, 'w') as file:\n",
    "                file.write(\"tmp\")\n",
    "        self.dev = xnvme.xnvme_file_open(char_pointer_cast(file_path), byref(opts))\n",
    "        size = IMAGENET_MAX_SIZE\n",
    "        self.buffers = (c_void_p * self.batch_size)()\n",
    "        self.views = []\n",
    "        for i in range(self.batch_size):\n",
    "            buf = xnvme.xnvme_buf_alloc(self.dev, size)\n",
    "            view = np.ctypeslib.as_array(\n",
    "                    cast(buf, POINTER(c_uint8)),\n",
    "                    shape=(size,),\n",
    "            )\n",
    "            self.buffers[i] = cast(buf, c_void_p)\n",
    "            self.views.append(view)\n",
    "\n",
    "        self.n = len(self.files)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        shuffle(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        batch = []\n",
    "        labels = []\n",
    "\n",
    "        if self.i >= self.n:\n",
    "            self.__iter__()\n",
    "            raise StopIteration\n",
    "        \n",
    "        opts = xnvme.xnvme_opts()\n",
    "        xnvme.xnvme_opts_set_defaults(byref(opts))\n",
    "        for i in range(self.batch_size):\n",
    "            jpeg, label = self.files[self.i % self.n]\n",
    "                \n",
    "            file = xnvme.xnvme_file_open(char_pointer_cast(jpeg), byref(opts))\n",
    "            size = xnvme.xnvme_dev_get_geo(file).contents.tbytes\n",
    "            ctx = xnvme.xnvme_file_get_cmd_ctx(file)\n",
    "            err = xnvme.xnvme_file_pread(byref(ctx), self.buffers[i], size, 0)\n",
    "            if err:\n",
    "                raise IOError(f\"Err: {err}\")\n",
    "            \n",
    "            batch.append(\n",
    "                self.views[i][:size]    \n",
    "            )\n",
    "           \n",
    "            labels.append(\n",
    "                torch.tensor([label], dtype=torch.int32)\n",
    "            )\n",
    "            xnvme.xnvme_file_close(file)\n",
    "            \n",
    "            self.i += 1\n",
    "        \n",
    "        return (batch, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "\n",
    "    def __del__(self):\n",
    "        for i in range(self.batch_size):\n",
    "            xnvme.xnvme_buf_free(self.dev, self.buffers[i])\n",
    "        xnvme.xnvme_file_close(self.dev)\n",
    "\n",
    "    next = __next__\n",
    "\n",
    "@pipeline_def\n",
    "def xnvme_file_pipe(datadir):\n",
    "    pipe = Pipeline.current()\n",
    "    batch_size = pipe.max_batch_size\n",
    "\n",
    "    jpegs, labels = fn.external_source(\n",
    "        source=XNVMEFileInputIterator(datadir=datadir, batch_size=batch_size), num_outputs=2, dtype=[types.UINT8, types.INT32],\n",
    "    )\n",
    "    images = fn.decoders.image(jpegs,\n",
    "        device=\"mixed\",\n",
    "        output_type=types.RGB,\n",
    "        device_memory_padding=211025920,\n",
    "        host_memory_padding=140544512,\n",
    "    )\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227606ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNVMECPUInputIterator(object):\n",
    "    # This is implemented using the xNVMe GDS-like backend with CPU buffers\n",
    "    def __init__(self, dev, bmap, batch_size):\n",
    "        with open(bmap) as f:\n",
    "            d = json.load(f)\n",
    "            dirs = sorted(list(set([k.split(\"/\")[1] for k in d.keys()])))\n",
    "            for k in d.keys():\n",
    "                d[k][\"label\"] = dirs.index(k.split(\"/\")[1])\n",
    "            self.files = list(d.values())\n",
    "        \n",
    "        self.dev = dev\n",
    "        self.batch_size = batch_size\n",
    "        self.nsid = xnvme.xnvme_dev_get_nsid(self.dev)\n",
    "        geo = xnvme.xnvme_dev_get_geo(self.dev)\n",
    "        self.nlb = NLB\n",
    "        self.nbytes = geo.contents.nbytes\n",
    "        size = IMAGENET_MAX_SIZE \n",
    "        self.qd = GDS_QD\n",
    "        self.buffers = (c_void_p * self.batch_size)()\n",
    "        self.views = []\n",
    "        for i in range(self.batch_size):\n",
    "            buf = xnvme.xnvme_buf_alloc(dev, size)\n",
    "            view = np.ctypeslib.as_array(\n",
    "                    cast(buf, POINTER(c_uint8)),\n",
    "                    shape=(size,),\n",
    "            )\n",
    "            self.buffers[i] = cast(buf, c_void_p)\n",
    "            self.views.append(view)\n",
    "        self.queue = POINTER(xnvme.xnvme_queue)()\n",
    "        err = xnvme.xnvme_queue_init(self.dev, self.qd, 0, byref(self.queue))\n",
    "        if err:\n",
    "            raise RuntimeError(f\"Failed to init queue: {err}\")\n",
    "        \n",
    "        self.n = len(self.files)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        shuffle(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        labels = []\n",
    "\n",
    "        if self.i >= self.n:\n",
    "            self.__iter__()\n",
    "            raise StopIteration\n",
    "\n",
    "        slbas = (c_uint32 * self.batch_size)()\n",
    "        elbas = (c_uint32 * self.batch_size)()\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            file = self.files[self.i % self.n]\n",
    "            labels.append(\n",
    "                torch.tensor([file[\"label\"]], dtype=torch.int32)\n",
    "            )\n",
    "            slbas[i] = c_uint32(file[\"startblock\"])\n",
    "            elbas[i] = c_uint32(file[\"endblock\"])\n",
    "            self.i += 1\n",
    "        \n",
    "        err = xnvme.xnvme_io_range_submit(self.queue, xnvme.XNVME_SPEC_NVM_OPC_READ, slbas, elbas, self.nlb, self.nbytes, self.buffers, self.batch_size)\n",
    "        if err:\n",
    "            raise IOError(f\"Err: {err}\")\n",
    "\n",
    "        batch = [self.views[i][:((elbas[i] - slbas[i])+ 1) * self.nbytes] for i in range(self.batch_size)]\n",
    "        return (batch, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __del__(self):\n",
    "        for i in range(self.batch_size):\n",
    "            xnvme.xnvme_buf_free(self.dev, self.buffers[i])\n",
    "        xnvme.xnvme_queue_term(self.queue)\n",
    "\n",
    "    next = __next__\n",
    "\n",
    "@pipeline_def\n",
    "def xnvme_gds_cpu_pipe(dev, bmap):\n",
    "    pipe = Pipeline.current()\n",
    "    batch_size = pipe.max_batch_size\n",
    "\n",
    "    jpegs, labels = fn.external_source(\n",
    "        source=XNVMECPUInputIterator(dev=dev, bmap=bmap, batch_size=batch_size), num_outputs=2, dtype=[types.UINT8, types.INT32]\n",
    "    )\n",
    "    images = fn.decoders.image(jpegs,\n",
    "        device=\"mixed\",\n",
    "        output_type=types.RGB,\n",
    "        device_memory_padding=211025920,\n",
    "        host_memory_padding=140544512,\n",
    "    )\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac94e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNVMEGPUInputIterator(object):\n",
    "    # This is implemented using the xNVMe GDS-like backend with GPU buffers\n",
    "    def __init__(self, dev, bmap, batch_size):\n",
    "        with open(bmap) as f:\n",
    "            d = json.load(f)\n",
    "            dirs = sorted(list(set([k.split(\"/\")[1] for k in d.keys()])))\n",
    "            for k in d.keys():\n",
    "                d[k][\"label\"] = dirs.index(k.split(\"/\")[1])\n",
    "            self.files = list(d.values())\n",
    "        \n",
    "        self.dev = dev\n",
    "        self.batch_size = batch_size\n",
    "        self.nsid = xnvme.xnvme_dev_get_nsid(self.dev)\n",
    "        geo = xnvme.xnvme_dev_get_geo(self.dev)\n",
    "        self.nlb = NLB\n",
    "        self.nbytes = geo.contents.nbytes\n",
    "        size = IMAGENET_MAX_SIZE\n",
    "        self.qd = GDS_QD\n",
    "        self.buffers = (c_void_p * self.batch_size)()\n",
    "        self.views = []\n",
    "        for i in range(self.batch_size):\n",
    "            buf = xnvme.xnvme_buf_alloc(dev, size)\n",
    "            self.buffers[i] = cast(buf, c_void_p)\n",
    "            view = cp.ndarray(shape=(size,), dtype=np.uint8, memptr=cp.cuda.MemoryPointer(cp.cuda.UnownedMemory(buf, size, dev), 0))\n",
    "            self.views.append(view)\n",
    "        self.queue = POINTER(xnvme.xnvme_queue)()\n",
    "        err = xnvme.xnvme_queue_init(self.dev, self.qd, 0, byref(self.queue))\n",
    "        if err:\n",
    "            raise RuntimeError(f\"Failed to init queue: {err}\")\n",
    "        \n",
    "        self.n = len(self.files)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        shuffle(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        labels = []\n",
    "\n",
    "        if self.i >= self.n:\n",
    "            self.__iter__()\n",
    "            raise StopIteration\n",
    "\n",
    "        slbas = (c_uint32 * self.batch_size)()\n",
    "        elbas = (c_uint32 * self.batch_size)()\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            file = self.files[self.i % self.n]\n",
    "            labels.append(\n",
    "                torch.tensor([file[\"label\"]], dtype=torch.int32)\n",
    "            )\n",
    "            slbas[i] = c_uint32(file[\"startblock\"])\n",
    "            elbas[i] = c_uint32(file[\"endblock\"])\n",
    "            self.i += 1\n",
    "        \n",
    "        err = xnvme.xnvme_io_range_submit(self.queue, xnvme.XNVME_SPEC_NVM_OPC_READ, slbas, elbas, self.nlb, self.nbytes, self.buffers, self.batch_size)\n",
    "        if err:\n",
    "            raise IOError(f\"Err: {err}\")\n",
    "\n",
    "        batch = [self.views[i][:((elbas[i] - slbas[i])+ 1) * self.nbytes] for i in range(self.batch_size)]\n",
    "        return (batch, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __del__(self):\n",
    "        for i in range(self.batch_size):\n",
    "            xnvme.xnvme_buf_free(self.dev, self.buffers[i])\n",
    "        xnvme.xnvme_queue_term(self.queue)\n",
    "\n",
    "    next = __next__\n",
    "\n",
    "@pipeline_def\n",
    "def xnvme_gds_gpu_pipe(dev, bmap):\n",
    "    pipe = Pipeline.current()\n",
    "    batch_size = pipe.max_batch_size\n",
    "\n",
    "    jpegs, labels = fn.external_source(\n",
    "        source=XNVMEGPUInputIterator(dev=dev, bmap=bmap, batch_size=batch_size), num_outputs=2, dtype=[types.UINT8, types.INT32]\n",
    "    )\n",
    "    images = fn.decoders.image(jpegs,\n",
    "        device=\"mixed\",\n",
    "        output_type=types.RGB,\n",
    "        device_memory_padding=211025920,\n",
    "        host_memory_padding=140544512,\n",
    "    )\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50f7a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XNVMEBAMInputIterator(object):\n",
    "    # This is implemented using the xNVMe BaM-like backend\n",
    "    def __init__(self, dev, bmap, batch_size):\n",
    "        with open(bmap) as f:\n",
    "            d = json.load(f)\n",
    "            dirs = sorted(list(set([k.split(\"/\")[1] for k in d.keys()])))\n",
    "            for k in d.keys():\n",
    "                d[k][\"label\"] = dirs.index(k.split(\"/\")[1])\n",
    "            self.files = list(d.values())\n",
    "        \n",
    "        self.dev = dev\n",
    "        self.batch_size = batch_size\n",
    "        self.nsid = xnvme.xnvme_dev_get_nsid(self.dev)\n",
    "        geo = xnvme.xnvme_dev_get_geo(self.dev)\n",
    "        self.nlb = NLB\n",
    "        self.nbytes = geo.contents.nbytes\n",
    "        size = IMAGENET_MAX_SIZE\n",
    "        buffers = []\n",
    "        self.views = []\n",
    "        for i in range(self.batch_size):\n",
    "            buf = xnvme.xnvme_buf_alloc(dev, size)\n",
    "            buffers.append(buf)\n",
    "            view = cp.ndarray(shape=(size,), dtype=np.uint8, memptr=cp.cuda.MemoryPointer(cp.cuda.UnownedMemory(buf, size, dev), 0))\n",
    "            self.views.append(view)\n",
    "            \n",
    "        self.buffers = np.asarray(buffers)\n",
    "        self.buffers_ref = self.buffers.ctypes.data_as(POINTER(c_void_p))\n",
    "        self.n = len(self.files)\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        shuffle(self.files)\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        labels = []\n",
    "\n",
    "        if self.i >= self.n:\n",
    "            self.__iter__()\n",
    "            raise StopIteration\n",
    "\n",
    "        slbas = np.zeros([self.batch_size], np.uint32)\n",
    "        elbas = np.zeros([self.batch_size], np.uint32)\n",
    "\n",
    "        for i in range(self.batch_size):\n",
    "            file = self.files[self.i % self.n]\n",
    "            labels.append(\n",
    "                torch.tensor([file[\"label\"]], dtype=torch.int32)\n",
    "            )\n",
    "            slbas[i] = file[\"startblock\"]\n",
    "            elbas[i] = file[\"endblock\"]\n",
    "            \n",
    "            self.i += 1\n",
    "\n",
    "        y = 64\n",
    "        x = 2048        \n",
    "        err = xnvme.xnvme_kernels_range_submit(x, y, self.dev, xnvme.XNVME_SPEC_NVM_OPC_READ, slbas.ctypes.data_as(POINTER(c_uint32)), elbas.ctypes.data_as(POINTER(c_uint32)), self.nlb, self.nbytes, self.buffers_ref, self.batch_size)\n",
    "        if err:\n",
    "            raise IOError(f\"Err: {err}\")\n",
    "\n",
    "        batch = [self.views[i][:((elbas[i] - slbas[i])+ 1) * self.nbytes] for i in range(self.batch_size)]\n",
    "        return (batch, labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    def __del__(self):\n",
    "        for i in range(self.batch_size):\n",
    "            xnvme.xnvme_buf_free(self.dev, int(self.buffers[i]))\n",
    "\n",
    "    next = __next__\n",
    "    \n",
    "@pipeline_def\n",
    "def xnvme_bam_pipe(dev, bmap):\n",
    "    pipe = Pipeline.current()\n",
    "    batch_size = pipe.max_batch_size\n",
    "\n",
    "    jpegs, labels = fn.external_source(\n",
    "        source=XNVMEBAMInputIterator(dev=dev, bmap=bmap, batch_size=batch_size), num_outputs=2, dtype=[types.UINT8, types.INT32]\n",
    "    )\n",
    "    images = fn.decoders.image(jpegs,\n",
    "        device=\"mixed\",\n",
    "        output_type=types.RGB,\n",
    "        device_memory_padding=211025920,\n",
    "        host_memory_padding=140544512,\n",
    "    )\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fd0068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reference using DALI fn.readers.file\n",
    "with dali_pipe(datadir=DATADIR, batch_size=BATCH_SIZE, num_threads=1, device_id=0) as pipe:\n",
    "    show_pipeline_output(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287ecc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with fn.readers.file with external source\n",
    "with file_pipe(datadir=DATADIR, batch_size=BATCH_SIZE, num_threads=2, device_id=0) as pipe:\n",
    "    show_pipeline_output(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53faab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using xNVMe file with the external source\n",
    "with xnvme_file_pipe(datadir=DATADIR, batch_size=BATCH_SIZE, num_threads=1, device_id=0) as pipe:\n",
    "    show_pipeline_output(pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93c2ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using xNVMe with GDS-like backend (CPU buffers)\n",
    "opts = xnvme.xnvme_opts()\n",
    "xnvme.xnvme_opts_set_defaults(byref(opts))\n",
    "opts.be = char_pointer_cast(\"gds\")\n",
    "opts.mem = char_pointer_cast(\"cpu\")\n",
    "dev = xnvme.xnvme_dev_open(char_pointer_cast(\"/dev/libnvm0\"), byref(opts))\n",
    "if not dev:\n",
    "    raise RuntimeError(\"xNVMe failed to open device\")\n",
    "xnvme.xnvme_dev_derive_geo(dev)\n",
    "with xnvme_gds_cpu_pipe(dev=dev, bmap=BMAP, batch_size=BATCH_SIZE, num_threads=1, device_id=0) as pipe:\n",
    "    show_pipeline_output(pipe)\n",
    "xnvme.xnvme_dev_close(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a8d3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using xNVMe with GDS-like backend (GPU buffers)\n",
    "opts = xnvme.xnvme_opts()\n",
    "xnvme.xnvme_opts_set_defaults(byref(opts))\n",
    "opts.be = char_pointer_cast(\"gds\")\n",
    "opts.mem = char_pointer_cast(\"gpu\")\n",
    "dev = xnvme.xnvme_dev_open(char_pointer_cast(\"/dev/libnvm0\"), byref(opts))\n",
    "if not dev:\n",
    "    raise RuntimeError(\"xNVMe failed to open device\")\n",
    "xnvme.xnvme_dev_derive_geo(dev)\n",
    "with xnvme_gds_gpu_pipe(dev=dev, bmap=BMAP, batch_size=BATCH_SIZE, num_threads=1, device_id=0) as pipe:\n",
    "    show_pipeline_output(pipe)\n",
    "xnvme.xnvme_dev_close(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89609f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using xNVMe with BaM-like backend\n",
    "opts = xnvme.xnvme_opts()\n",
    "xnvme.xnvme_opts_set_defaults(byref(opts))\n",
    "opts.be = char_pointer_cast(\"bam\")\n",
    "dev = xnvme.xnvme_dev_open(char_pointer_cast(\"/dev/libnvm0\"), byref(opts))\n",
    "if not dev:\n",
    "    raise RuntimeError(\"xNVMe failed to open device\")\n",
    "xnvme.xnvme_dev_derive_geo(dev)\n",
    "with xnvme_bam_pipe(dev=dev, bmap=BMAP, batch_size=BATCH_SIZE, num_threads=1, device_id=0) as pipe:\n",
    "     show_pipeline_output(pipe)\n",
    "xnvme.xnvme_dev_close(dev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
